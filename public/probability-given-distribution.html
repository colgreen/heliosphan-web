<!DOCTYPE html>
<html lang="en">

<head>
    <title>Gradient of Generative Models</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML-full'></script>
    <link rel="stylesheet" href="stuff.css" type="text/css" media="screen" />
</head>

<body>
    <div class="bannercolumn">
        <a href="index.html">
            <img src="banner_v5_thin.jpg" style="display: block; border-radius: 6px" alt="Welcome to Heliosphan" />
        </a>
    </div>
    <div class="articlebodyouter">
        <div class="articlebodyinner">
            <br />
            <h2 align="center">Gradient of Generative Models</h2>
            <br />
            <p>
                Consider a very simple generative model consisting of a single variable B representing the probability of a biased coin toss landing heads up.
                We are given a data set consisting of a seqence of actual coin tosses and we are tasked with fitting the model to the data, i.e. finding 
                the value of B that best represents the data (or more specifically - that best fits the system from which the data was sampled).
            </p>
            <p>
                Let's start with a simple data set of H-H-T-H. We observe that 75% of the outcomes are heads, and therefore
                conclude that the optimal value for B is 0.75. Stated formally:
            </p>
            <p>$$B = \frac{N_{heads}}{N_{heads} + N_{tails}} \tag{1}$$</p>
            <p>I.e. We set B to the proportion of total tosses that are heads.</p>

            <p><em>Note. This estimate based on a simple ratio isn't quite right, however the error will diminish with an increasing number of samples.</em></p>
            <p>
                If we now run our model in generative mode (i.e. take random samples from it) then we see that approximately 75% of the tosses are heads as per the 
                real coin, i.e. the distribution of our model matches that of the system it is modelling. So far, so good.
            </p>
            <br/>
            <b>Big Models</b>
            <p>
                Unfortunately our coin flip model with a single variable isn't very useful. If we wish to model large datasets of images, audio, text, etc. and the complex
                hierarchy of structures inherent to those types of data (e.g. understanding that some subset of pixels in an image represent an animal that
                is related to other objects and animals in the same scene - such as a kitten playing with a ball of string), then we need a lot more model variables.
                Contemporary models may contain 100s of millions or even billions of variables, and there is no simple mathematical expression (such as our equation 1)
                that will give us the optimal value for each of them. In such cases the go-to method is that of gradient following
                (most commonly referred to as <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>).
            </p>
            <br/>
            <b>Gradient Descent</b>
            <p>
                The basic idea is that we have some means of measuring the quality of a given model with respect to some goal, i.e. what we want
                the model to do. Using such a quality metric we can determine, for any given single model parameter, how the model quality changes with 
                small changes to that parameter. We can then make lots of tiny incremental updates to the each of the millions of parameters and
                slowly 'nudge' the model towards the desired goal. It turns out this approach can work very well, although caveats apply. 
            </p>
            <p>
                In short, we calculate the gradient (i.e. the first order derivative) of the quality metric with respect to a parameter and follow 
                the gradient in tiny steps. That is, rather than the equation 1 approach of jumping directly to the know optimal value, instead we move slowly
                in the direction where we think the optimal value will be, but each variable is moving simultaneously with millions of others
                so our gradient estimate for each variable is changing as all of those other variables change.  
            </p>

            <br/>
            <b>Supervised Models</b>
            <p>
                 In <a href="https://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning</a> a model will typically produce 
                an output vector for a given input vector, and the training data is in the form of input-output pairs. In that world there's a natural choice
                of quality - how close each produced output vector is to the desired (target) output vector. The error at the outputs is a simple pointwise subtraction
                of the target vector from the produced vector, it's then necessary to communicate that error back through the model in order to calculate
                the error gradient at each variable. As we go further back into layers of the model the error signal can become greatly diminished and noisy, 
                and this has been a long running issue and topic of research with error <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>.
            </p>
            <p>
                The need for target vectors also causes problems, e.g. for the inputs we might have a set of raw images, and for the corresponding outputs we migth 
                have a series of labels of things (i.e. binary categories) for each image (a ball of string, a kitten...).
                The choice of labels will typically be somewhat artifical in that they're defined by people and what they think the relevant features are in each image,
                but the choice of labels may not be fully representative of everything 'going on'
                in an image that is relevant to the model we wish to build, after all a picture is worth a thousand words
                
                
                 e.g. if we ant to build a genral purpose image recognition model then we want to model
                everything in an image - the fluffiness of the cat, the texture of the carpet, the subtle chaneg in light tone across a room, the wound fibres that
                make up the string, the relative positions of the cat and and the ball of string... and so on.
            </p>





            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/> 
                
                
                
                
                



            </p>


                
                
                we present a model with a 
                inputs and some desried outputs, and 
                , inputs and outputs, and attempt to get some
                inputs, and the outputs they relate to and we can obtai
                
                
                (i.e. any given set of model parameters)
                and using this we can determine, for any given single parameter, how the model quality changes with small changes to that parameter. We can then 
                make lots of tiny incremental updates to the each of the millions of parameters and in doing so we can often find good models. In short, we calculate 
                the gradient (i.e. the first order derivative) of the quality metric with respect to a parameter, all other parameters remaining fixed for the purposes
                of each gradient calculation.</p> 
                
                
                increasing or dereasing that parameter 
                
                ich direction causes the model quality to increase 
                
                
                 of the model's quality and we can therefore</p>
            <p>
                

            </p>
                
                
                

            
            
<br/>
<br/>

<br/>
<br/>
<br/>
<br/>
<br/>
            





            <p>
                There follows a series of notes and annotations to
                <a href="http://sifter.org/~simon/journal/20150603.html">Gradient of Auto-Normalized Probability Distributions</a>
            </p>
            <br/>
            <b>Introduction</b>
            <p>
                This post is an exploration of some of the maths in furf networks (see: <a href="http://sifter.org/~simon/fusion-reflection.html">Fusion-Reflection</a>). 
                A furf net is a <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> that is capable of generating and completing 
                patterns stochastically, and also learning patterns we present to it.
            </p>
            <b>Furf Nets</b>
            <p>
                A furf network consists of a network of nodes arranged into layers. Nodes within each layer are arranged into groups, and 
                a node can be in only one group. Each group represents a variable with N possible discrete states where N is the number of 
                nodes in the group, i.e. each node in the group represents one of the possible states.
            </p>
            <p>
                We determine a group's state by summing all incoming activation signals to nodes in a group to obtain one activation 
                level per node, and then normalising those activations across the group such that they sum to one. We can then select the winning
                node stochastically by treating each node's activation level as its probability of activation.  
            </p>
            <p>
                In <a href="http://sifter.org/~simon/fusion-reflection.html">Fusion-Reflection</a> the groups are arranged as a tree, i.e. 
                the nodes in a group are fully connected to all the nodes in one parent group, and a group can have multiple child groups. Although 
                the furf generative model and learning law are powerful, the restriction to a tree topology limits the expressibility of the
                generative model and therefore the type of abstract associations that can be learned. As such there is good motivation to extend furf trees
                to furf networks in which a group can be fully connected to multiple parent groups.
            </p>

            <b>Product of Experts</b>
            <p>
                A key problem in extending furf trees to networks is that the learning law requires each node's activation probability, i.e. its
                post normalised activation level. In the tree model each node has only one incoming activation (only one node in the parent group 
                is active at a time), so we can devise a learnign law that 

            </p>



            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>


            <p>
                Incoming connections to a node (and the connection weights) serve to define that node's activation level and thus its probability
                of being 'chosen' (being the selected state for the variable). There is a problem therefore around how to
                ensure that the activation levels sum to 1.0 (are properly normalised) when propagating signals through a
                network, and in turn the nature of the weight update rule(s) - should we (or indeed can we) define an update
                rule that ensures that activation levels are 'pre- normalised' by a clever choice of weights? Or should we
                'post-normalise' the activation levels?
            </p>
            <p>
                <a href="http://sifter.org/~simon/journal/20150603.html">Gradient of Auto-Normalized Probability Distributions</a>                is working on the basis that the activation levels are not naturally normalised and must therefore be post-normalised
                before stochastically selecting a state/node within a group. As such we arrive at equation (1) (I will refer
                to the equation numbers used in the source article, and will annotate those equations with intermediate steps/equations
                numbered accordingly).
            </p>
            <br/>
            <b>Annotations</b>
            <p>Note. Written content from the original/source document will be shown indented and in <i>italic</i>. My words
                are in regular font.</p>
            <br />
            <blockquote><i>Consider a model which is an auto-normalized probability distribution over the states of a discrete variable D:</i></blockquote>

            <p>$$\mathbf{P}(D_k) \equiv \frac{F_k}{\sum_m F_m}\tag{1}$$</p>

            <blockquote><i>where each `F_k` is a scalar valued function of some (implicit) parameters.</i></blockquote>
            <br />
            <p>
                So `F_k` here is the activation level for one node; It is being normalised by dividing by the sum of all node activation
                levels within a group (hence `m` is just an arbitrarily chosen symbol, i.e. `m` has no relation to `k`).
                Hence each `F_k` is the result of combining input signals to a node in whatever way we choose to do that
                (the maths presented is agnostic of how that combining is done).
            </p>
            <p><b>N.B.</b>&nbsp;&nbsp;&nbsp;$\mathbf{P}(D_k)$ could be more formally written as $\mathbf{P}(D_k | M)$, where
                M is the model defined by the right hand side of the equation. However, to minimise visual clutter the conditional
                part is omitted throughtout the original document (and this one).</p>

            <br />
            <blockquote><i>if `del_i` is the partial derivative with respect to some variable that only affects `F_i` then:</i></blockquote>
            <p>$$ \partial_i \log \mathbf{P}(D_k) = \frac{\partial_i F_k}{F_k} - \frac{\partial_i F_i}{\sum_m F_m} \tag{2}$$</p>

            <blockquote><i>where `del_i F_k = 0` when `i != k`.</i></blockquote>
            <br />
            <p>
                There are a few things going on here. `del_i` is partial derivative notation, so `del_i f` would be read as "the rate of
                change of `f` with respect to `i`". In the source document `i` is used as shorthand to refer to some variable
                feeding into `F_i` (e.g. a signal being communicated to a node via a connection), hence `del_i` could perhaps
                be more formally written as `del_{v_i}`, so we're just avoiding the extra visual clutter and the small hard
                to read 'double subscript' `i`.
            </p>

            <p>There now follows a number of intermediate steps between equations (1) and (2).</p>
            <p>
                \begin{align} \log \mathbf{P}(D_k) & = \log \frac{F_k}{\sum_m F_m} \tag{1.1} \\\\ & = \log F_k - \log \sum_m F_m \tag{1.2}
                \\\\ \partial_i \log \mathbf{P}(D_k) & = \partial_i \log F_k - \partial_i \log \sum_m F_m \tag{1.3} \\\\
                & = \frac{\partial_i F_k}{F_k} - \frac{\partial_i \sum_m F_m}{\sum_m F_m} \tag{1.4} \end{align}
            </p>
            <br />
            <p>
                Regarding the hop from (1.3) to (1.4), note that $\partial_x{log(x)} = 1/x$, that explains the denominator in both right
                hand side (RHS) terms in (1.4). The numerator comes about by application of the chain rule; Taking the first
                RHS term as an example, we have $1/F_k$ which is the derivative w.r.t $F_k$, but we wish to find the derivative
                w.r.t $i$, hence we multiply by the derivative of $F_k$ w.r.t $i$ which is written as $\partial_i F_k$. This
                same process is applied to the second RHS term.
            </p>
            <p>We now note that $\partial_i{F_m} = 0$ when $i \neq m$, hence (1.4) simplifies to...</p>

            <p>$$ \partial{_i} \log \mathbf{P}(D_k) = \frac{\partial_i F_k}{F_k} - \frac{\partial_i F_i}{\sum_m F_m} \tag{2}$$</p>
            <br />

            <blockquote><i>Separately, if we have some true data $\bar{D}$ (a large set of observed states of `D`, like throws of a die), expressed as a normalized 
            distribution by $\mathbf{\bar{P}}(\bar{D})$, then we can compute our model's probability of generating that observed data (per-sample, so independent of the size
            of `\bar{D}`) as:</i></blockquote>

            <p>$$ \mathbf{P}(\bar{D}) = \prod_k \mathbf{P}(D_k)^{\bar{\mathbf{P}}(\bar{D}_k)} \tag{3}$$</p>

            <blockquote><i>The log of which is the cross entropy:</i></blockquote>

            <p>$$ \log \mathbf{P}(\bar{D}) = \sum_k [ \bar{\mathbf{P}}(\bar{D}_k) \log \mathbf{P}(D_k) ] \tag{4}$$</p>

            <p>Regarding $\bar{\mathbf{P}}(\bar{D}_k)$, $\bar{\mathbf{P}}$ is accessing the various (`k`) states of the observed
                distribution $\bar{D}$ to yield their observed probability (i.e. relative frequency). This could be more
                formally be written as just $\mathbf{P}(\bar{D}_k)$, i.e. the unconditional probability of state $\bar{D}_k$.
                However, we've omitted the condition elsewhere to avoid visual clutter (see above note), hence we now need
                to emphasise that there is no conditional part. In summary $\mathbf{P}(\bar{D})$ is shorthand for $\mathbf{P}(\bar{D}
                | M)$ (where $M$ is our model), i.e. the probability that our model will generate outputs with distribution
                $\bar{D}$. $\bar{\mathbf{P}}(\bar{D}_k)$ is a our customised notation for $\mathbf{P}(\bar{D}_k)$.
            </p>
            <p>Note that equation (4) is indeed the formula for cross entropy, but without the minus sign that it is normally
                shown with. Generally we wish to minimise
                <a href="crossentropy.html">cross entropy</a> to find better models, so here we just need to note that the
                sign is flipped (we would want to <i>maximise</i> equation 4 as it is written).
            </p>
            <br />
            <blockquote><i>Taking the partial derivative and pulling in `\partial_i \log \mathbf{P}(D_k)` from above:</i></blockquote>
            <p>
                \begin{align} \partial_i \log \mathbf{P}(\bar{D}) & = \sum_k{ \left[ \bar{\mathbf{P}}(\bar{D}_k) \partial_i \log \mathbf{P}(D_k)
                \right]} \tag{4.5} \\\\ & = \sum_k{\left[ \bar{\mathbf{P}}(\bar{D}_k) \left(\frac{\partial_i F_k}{F_k} -
                \frac{\partial_i F_i}{\sum_m F_m}\right) \right]} \tag{4.6} \\\\ & = \sum_k{\left[ \bar{\mathbf{P}}(\bar{D}_k)
                \frac{\partial_i F_k}{F_k} \right]} - \sum_k{\left[ \bar{\mathbf{P}}(\bar{D}_k) \frac{\partial_i F_i}{\sum_m
                F_m} \right]} \tag{4.7} \\\\ & = \bar{\mathbf{P}}(\bar{D}_i) \frac{\partial_i F_i}{F_i} - \frac{\partial
                F_i}{\sum_m F_m} \tag{4.8} \\\\ & = \frac{\partial_i F_i}{F_i} \left[ \bar{\mathbf{P}}(\bar{D}_i) - \frac{F_i}{\sum_m
                F_m}\right] \tag{4.9} \\\\ & = \frac{\partial_i F_i}{F_i} \left[ \bar{\mathbf{P}}(\bar{D}_i) - \mathbf{P}(D_i)
                \right] \tag{5} \end{align}
            </p>
            <br />
            <p>At (4.5) we take the derivative w.r.t. $i$ (actually $v_i$, see notes above). $\bar{\mathbf{P}}(\bar{D}_k)$ is
                a constant factor w.r.t. $i$, therefore that term can go outside of the partial dertivative for clarity.
            </p>
            <p>At (4.6) we substitute in equation (2), and at (4.7) we simply multiply out the terms.
            </p>
            <p>At (4.8) the sum over k in the left hand term has simplified because $\partial_i{F_k}$ is zero for all $k$ except
                when $k=i$. In the right hand term we note that in (4.7) there was no $k$ in $\frac{\partial_i{F_i}}{\sum_m{F_m}}$,
                and since $\bar{\mathbf{P}}(\bar{D}_k)$ sums to one the entire term simplifies to $\frac{\partial_i{F_i}}{\sum_m{F_m}}$.
            </p>
            <p>At (4.9) $\partial_i{F_i} / F_i$ is factored out from both terms.
            </p>
            <p>At (5) The right hand term is substituted for the LHS of equation (1).
            </p>
            <br />

            <blockquote><i>In sum:</i></blockquote>
            <p>
                \begin{align} \mathbf{P}(D_k) & \equiv \frac{F_k}{\sum_m F_m} \\\\ \mathbf{P}(\bar{D}) & = \prod_k{ \mathbf{P}(D_k)^{\bar{\mathbf{P}}(\bar{D}_k)}
                } \\\\ \partial_i \log \mathbf{P}(\bar{D}) & = \frac{\partial_i F_i}{F_i} \left[ \bar{\mathbf{P}}(\bar{D}_i)
                - \mathbf{P}(D_i) \right] \tag{6} \end{align}
            </p>

            <blockquote><i>So, what's it mean and why is it interesting?</i></blockquote>

            <blockquote><i>Equation (1) is a fairly common scenario when you have some ability to model the relative likelihood or frequency of events. The denominator normalizes
            those relative votes into an actionable probability distribution.</i></blockquote>

            <blockquote><i>Equation (3) in most cases is the definitive measure of the quality of a model, and is thus usually the quantity you want to maximize.</i></blockquote>

            <blockquote><i>Equation (5) puts them together with the interesting result that the optimizing gradient is proportional to the residual error we get by subtracting
            our predicted distribution from the observed one. Note because these are probabilities, the components of that residual are nicely bounded from -1 to 1, so it has 
            the makings of a very well-behaved gradient. It is intuitively appealing because it is zero when our prediction matches observation. And it's just surprisingly
            clean and simple.</i></blockquote>

            <blockquote><i>Where it goes from here depends on the nature of the underlying F functions.</i></blockquote>

            <hr />

            <p>Continuing...</p>
            <blockquote><i>One case to consider is where the F distribution is a product of distributions (where each $G_j$ is a non-normalized pseudo 
            probability distribution over i):</i></blockquote>

            $$ F_i = \prod_j{G_{j,i}} \tag{7}$$

            <p>So this is e.g. where we have multiple input connections to a node and we choose to combine them by multiplying,
                in the spirit of treating the inputs as independent probabilities where the joint probability of independent
                events A and B is given by $P(A,B) = P(A) \cdot P(B)$
            </p>

            <blockquote><i>Then:</i></blockquote>

            <p>$$ \partial_{j,i}F_i = \frac{F_i}{G_{j,i}} \partial_{j,i}{G_{j,i}} \tag{8} $$</p>
            <br />

            <p>As above, I'm interpreting $\partial_{j,i}$ as shorthand for $\partial_{v_{j,i}}$, i.e. the partial derivative
                w.r.t. some variable that affects $G_{j,i}$ (although I'm not 100% sure about that interpretation). Intermediate
                steps leading up to equation (8)...</p>

            <p>First we take the partial derivative w.r.t. $v_{j,i}$ (using the chain rule).</p>
            <p>$$ \partial_{j,i}F_i = \partial_{G_{j,i}}{F_i} \cdot \partial_{j,i}G_{j,i} \tag{7.9} $$</p>

            <p><b>N.B.</b> Only the ${j,i}$th component remains from the product term in (7) because that's the only component
                that affects $v_{j,i}$. We have then applied the chain rule around that remaining variable.</p>

            <p>We now note that $\partial_{G_{j,i}}{F_i}$ is the product of all the the $G_{j,i}$ components apart from the
                current one we're calculating the derivative for - because all of those other component are constant terms
                w.r.t. the rate of change we're computing. Hence...</p>

            <p>$$ \partial_{j,i}F_i = \frac{F_i}{G_{j,i}} \partial_{j,i}{G_{j_i}} \tag{8} $$</p>
            <br />

            <blockquote><i>Applying (5), note $F_i$ cancels out, so curiously we end up with essentially (5) again:</i></blockquote>

            <p>$$ \partial_{j,i}{log{ \mathbf{P}(\bar{D})}} = \frac{\partial_{j,i}G_{j,i}}{G_{j,i}} \left[\bar{\mathbf{P}}(\bar{D_i})
                - \mathbf{P}(D_i)\right] \tag{9} $$</p>

            <p>Here we've taken (5) and replaced the $\frac{\partial_i{F_i}}{F_i}$ term with equation (8). $F_i$ cancels out
                to give equation (9).</p>

            <blockquote><i>The interesting point here being that even though many $G_j$s are being combined into the final predicted distribution, the "explaining away"
            competition amongst them is coordinated entirely via the consolidated $\mathbf{P}(Di)$ predictions (as expressed in the error residual). That is, other than the
            common error residual, the gradient with respect to a parameter of some $G_{j,i}$ is local.</i></blockquote>

            <blockquote><i>Note this is analogous to the process that causes singular vectors to become orthogonal in the incremental SVD approach (when training
            them all at once as opposed to one by one). There we are removing the net projection of the current vectors from the input before training. Here we are 
            removing the net prediction. This reverse inhibition is a recurring theme in various learning algorithms, including some brain-inspired ones.</i></blockquote>
            <br />
            <p>Incremental SVD refers the matrix factorisation method whereby the side vectors are updated at each training
                data point. "when training them all at once as opposed to one by one", refers to 'simultaneous' SVD, whereby
                instead of factoring one pair of side vectors at a time (thus extracting the next most prominent feature
                in the data), we actually factor out multiple factors (side vectors) simultaneously. This is benefical because
                in the serial approach we can add the early vectors back into the error residuals (i.e. the remaining unexplained
                variance in the data) and re-learn them using the error residuals that have had later vectors extracted,
                this gives us a better set of early side vectors. In fact we can then go on to re-learn the later vectors,
                and so on. The logical conclusion to this cycling through previously learned vectors, adding them back to
                the residuals and re-learning them, is that what we really wanted to do all along was learn all of the side
                vectors at the same time, such that each pair of side vectors is responding to the learning going on in all
                of the other pairs. This results in the structures in the data being teased out gradually, and the side vector
                pairs each naturally settle on a feature to model that is orthoginal to all of the other features.
            </p>
            <br/>
            <p>Colin, June 2015</p>
            <br/>
            <hr />
            <div>
                <img src="creativecommons88x31.png" border="0" align="left" hspace="10" vspace="0" /> Copyright 2015 Colin
                Green.<br /> This article is licensed under a <a href="http://creativecommons.org/licenses/by/3.0/" rel="nofollow">Creative Commons
            Attribution 3.0 License
                </a>
                <br />
                <br />
            </div>
        </div>
    </div>
</body>

</html>