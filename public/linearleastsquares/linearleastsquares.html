<!DOCTYPE html>
<html lang="en">

<head>
  <title>Linear Least Squares</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=0.75" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML-full"></script>
  <link rel="stylesheet" href="../stuff.css" type="text/css" media="screen" />
  <link rel="stylesheet" href="../stuff-print.css" type="text/css" media="print" />
</head>

<body>
  <div class="bannercolumn">
    <a href="../index.html">
      <img src="../banner_v5_thin.jpg" style="display: block; border-radius: 6px" alt="Welcome to Heliosphan" />
    </a>
  </div>
  <div class="articlebodyouter">
    <div class="articlebodyinner">
      <br/>
      <h1 align="center">Linear Least Squares</h1>

      <br />
      <br /> Task: Given a set of points in 2D euclidean space, find the line that minimises squared error. A line is described
      by:
      <p style="font-size:large">$$ y = mx + c $$</p>
      <p>
        Therefore we wish to find the values of <b>m</b> and <b>c</b> that minimise squared error. The first step is to consider
        squared error as a function of <b>m</b> and <b>c</b>. I'll denote squared error with `E^2`.</p>
      <p style="font-size:large">$$ E^2 = f(m, c) $$</p>
      <br/>
      <p>So `E^2` is a 2D surface described by the above function. Due to the nature of squared error we know that the surface
        happens to be a convex function (bowl shaped) with a single minimum point. Therefore we can find that minimum point
        by finding the single (m,c) coordinate where the function's gradient is zero.
      </p>
      <p>So we need to obtain the gradient function and solve for a gradient of zero (see derivation below). Doing so gives
        the following functions for <b>m</b> and <b>c</b> respectively:</p>
      <br/>
      <p style="font-size:large">$$ m = \frac{n \sum_i{x_i y_i} - \sum_i{x_i} \sum_i{y_i}}{n \sum_i{x_i^2} - (\sum_i{x_i})^2} \tag{1}$$</p>
      <br/>
      <p style="font-size:large">$$ c = \frac{\sum_i{x_i} \sum_i{x_i y_i} - \sum_i{x_i^2} \sum_i{y_i}}{(\sum_i{x_i})^2 - n \sum_i{x_i^2}} \tag{2}$$</p>
      <br />
      <p>When evaluating the above functions be sure to evaluate the numerator term first and check for a zero, this avoids
        having to evaluate the denominator and also divide by zero errors.</p>
      <br />
      <p><i>
          Colin,
          <br /> October 2012
      </i></p>
      <br />
      <hr/>
      <b>Derivation - Page 1</b>
      <br/>
      <div style="text-align:center;">
        <img src="linearleastsquares_pg1.jpg" alt="linear least squares, derivation, page1" style="border:1px solid black" />
      </div>
      <br />
      <br />
      <br />
      <br />
      <b>Derivation - Page 2</b>
      <br/>
      <br/>
      <div style="text-align:center;">
        <img src="linearleastsquares_pg2.jpg" alt="linear least squares, derivation, page1" style="border:1px solid black" />
      </div>
      <br />
      <br />

      <br />
      <hr/>
      <div>
        <img src="../creativecommons88x31.png" border="0" align="left" hspace="10" vspace="0" /> Copyright 2012 Colin Green.<br/>        This article is licensed under a <a href="https://creativecommons.org/licenses/by/3.0/" rel="nofollow">
            Creative Commons
            Attribution 3.0 License
          </a>
        <br/>
        <br/>
      </div>
    </div>
  </div>
</body>

</html>