<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Cross Entropy</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=0.75"/>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {inlineMath: [['`','`'], ['\\(','\\)']]},
        });
    </script>
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML-full'></script>      
    <link rel="stylesheet" href="stuff.css" type="text/css" media="screen"/>
  </head>
  <body>
    <div class="bannercolumn">
        <a href="index.html">
            <img src="banner_v5_thin.jpg" style="display: block; border-radius: 6px" alt="Welcome to Heliosphan" />
        </a>
    </div>
    <div class="articlebodyouter">
      <div class="articlebodyinner">
        <br/>
        <h2 align="center">Cross Entropy</h2>
        <br />

        <p>Cross entropy is closely related to <a href="shannon-entropy.html">Shannon Entropy</a>:</p>
        <blockquote><i>  
          Shannon entropy is defined for a given discrete probability distribution; it measures how much information is required, on average, 
          to identify random samples from that distribution.
        </i></blockquote>

        <p>
          Shannon entropy does not define any coding scheme, but it does define how much information an optimal coding scheme for 
          a given distribution would use, on average, to identify random samples from that distribution.
        </p>
        <p>
          Consider two distributions, `D` and `\bar D`, each over the same set of states (or symbols), but with distinct probabilities assigned to those
          states (the letter 'D' is re-used to emphasise that these are distributions over the same set of states).
        </p>
        <p>
          Cross entropy is defined for the discrete probability distributions `D`, `\bar D`; it measures how much information is required, on average,
          to identify random samples from `\bar D` when using an optimal coding scheme constructed for `D`. To represent this mathematically we substitute
          `D` and `\bar D` into the Shannon entropy equation: 
        </p>
        <p>
            $$ H(\bar D, D) = - \sum_i{P(\bar D_i) \log{P(D_i)}} \tag{Cross entropy} $$
        </p>
        <p>
          Noting that the log expression in the Shannon entropy equation gives the optimal code length for a given probability, hence `D` is
          substituted in (i.e. the optimal coding scheme for `D` is in use); and the left-hand probability expression represents the probability 
          (or relative frequency) of each state, hence `\bar D` is substituted in there.
        </p>

        <br/>
        <b>Cross Entropy as a Distance or Error Metric</b>
        <p>
          If `\bar D` is identical to `D` then samples from `\bar D` are encoded optimally. If however `\bar D` differs from `D` then the average
          amount of information per sample increases above the minimum/optimal level defined by the Shannon entropy. Hence, the difference between the
          Shannon entropy and the cross entropy can be considered a distance metric with respect to two discrete distributions over the same set of states.
          The unit of distance is 'information per sample', i.e. the amount of additional information (in whatever units are preferred, e.g. bits, trits, etc.)
          required, on average, to identify a sample from `\bar D`.
        </p>
        <p>
          When training a generative model to produce a distribution matching some desired target distribution, the above cross entropy based metric
          is the natural choice and logically correct error metric for use in gradient following methods. Justification for this claim relates to redundant 
          information being a fundamental quantity we wish to minimise. I.e. if some model can reproduce a target distribution exactly, then that model has
          encoded everything necessary about the system being modelled in order to recreate the target distribution. If however the cross entropy is
          higher than the Shannon entropy, that is an indication that the model is wrong to some degree, and the difference describes the scale of that error
          in the fundamental units of how much information is required it.
        </p>
        <br />
        <p><i>
            Colin,
            <br /> October 20th, 2016
        </i></p>
        <br />
        <hr />
        <div>
            <img src="creativecommons88x31.png" border="0" align="left" hspace="10" vspace="0" /> Copyright 2016 Colin
            Green.
            <br /> This article is licensed under a <a href="http://creativecommons.org/licenses/by/3.0/" rel="nofollow">
                Creative Commons
                Attribution 3.0 License
            </a>
            <br />
            <br />
        </div>
      </div>
    </div>
  </body>
</html>
